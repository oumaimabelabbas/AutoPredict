{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c562e4a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 40...\n",
      "Page 40 : 36 annonces trouvées.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver \n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "path = r'C:\\Users\\HP830G5\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe'\n",
    "csv_file = r\"C:\\Users\\HP830G5\\Music\\cars_price_prediction\\data\\scrapped\\avito_cars_scrapping.csv\"\n",
    "\n",
    "labels = [\"Date Pub\", \"Ville\", \"Année-Modèle\", \"Boite de vitesses\", \"Type de carburant\",\n",
    "          \"Kilométrage\", \"Marque\", \"Modèle\", \"Nombre de portes\",\n",
    "          \"Origine\", \"Première main\", \"Puissance fiscale\", \"État\", \"Prix\"]\n",
    "\n",
    "def avito_scraper(page_num):\n",
    "    website = f'https://www.avito.ma/fr/maroc/v%C3%A9hicules?o={page_num}'\n",
    "    service = Service(executable_path=path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "    driver.get(website)\n",
    "    time.sleep(5)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    links = soup.find_all('a', class_='sc-1jge648-0 jZXrfL')\n",
    "    print(f\"Page {page_num} : {len(links)} annonces trouvées.\")\n",
    "\n",
    "    urls, prix_list, date_pub = [], [], []\n",
    "\n",
    "    for link in links:\n",
    "        if \"/voitures_d_occasion/\" in link.get('href', ''):\n",
    "            urls.append(link['href'])\n",
    "            prix_tag = link.find('span', class_='sc-3286ebc5-2 PuYkS')\n",
    "            prix_list.append(prix_tag.text.strip() if prix_tag else None)\n",
    "            date_tag = link.find('p', class_='sc-1x0vz2r-0 layWaX')\n",
    "            date_pub.append(date_tag.text.strip() if date_tag else None)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(urls)):\n",
    "        driver.get(urls[i])\n",
    "        try:\n",
    "            bouton = WebDriverWait(driver, 3).until(\n",
    "                EC.presence_of_element_located((By.XPATH, \"//button[@aria-label='Voir plus']\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView(true);\", bouton)\n",
    "            driver.execute_script(\"arguments[0].click();\", bouton)\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        html_link = driver.page_source\n",
    "        soup = BeautifulSoup(html_link, 'lxml')\n",
    "        blocs = soup.find_all('div', class_='sc-cd1c365e-2 bToMqV')\n",
    "\n",
    "        infos = {label: None for label in labels}\n",
    "        infos[\"Date Pub\"] = date_pub[i] if i < len(date_pub) else None\n",
    "        infos[\"Prix\"] = f\"{prix_list[i]} DH\" if i < len(prix_list) and prix_list[i] else None\n",
    "\n",
    "        # Récupérer la ville\n",
    "        ville_tag = soup.find('span', class_='sc-9ca53b09-15 eTxoKb')\n",
    "        if ville_tag:\n",
    "            ville_text = ville_tag.text.strip()\n",
    "            if ',' in ville_text:\n",
    "                infos[\"Ville\"] = ville_text.split(',')[1].strip()\n",
    "            else:\n",
    "                infos[\"Ville\"] = ville_text\n",
    "        else:\n",
    "            infos[\"Ville\"] = None\n",
    "\n",
    "        # Parcours des blocs pour remplir les autres informations\n",
    "        for bloc in blocs:\n",
    "            spans = bloc.find_all('span')\n",
    "            if len(spans) == 2:\n",
    "                valeur = spans[0].get_text(strip=True)\n",
    "                label = spans[1].get_text(strip=True)\n",
    "                if label in infos:\n",
    "                    infos[label] = valeur\n",
    "                    \n",
    "\n",
    "        # On ajoute seulement si un prix est disponible\n",
    "        if infos[\"Prix\"]:\n",
    "            data.append(infos)\n",
    "\n",
    "    print(f\"{len(data)} lignes valides trouvées sur cette page.\")\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # Écriture dans le CSV\n",
    "    if data:\n",
    "        with open(csv_file, \"a\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=labels)\n",
    "            if f.tell() == 0:\n",
    "                writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for i in range(40, 150):\n",
    "        print(f\"Scraping page {i}...\")\n",
    "        avito_scraper(i)\n",
    "        time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38913cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da70c1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
