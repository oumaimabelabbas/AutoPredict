{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c562e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "25 annonces trouvées\n",
      "  Voiture 1/25\n",
      "    Volkswagen - DH\n",
      "  Voiture 2/25\n",
      "    Mercedes-Benz - DH\n",
      "  Voiture 3/25\n",
      "    Volkswagen - DH\n",
      "  Voiture 4/25\n",
      "    Porsche - DH\n",
      "  Voiture 5/25\n",
      "    Volvo - DH\n",
      "  Voiture 6/25\n",
      "    Mercedes-Benz - DH\n",
      "  Voiture 7/25\n",
      "    Dacia - DH\n",
      "  Voiture 8/25\n",
      "    Audi - DH\n",
      "  Voiture 9/25\n",
      "    Renault - DH\n",
      "  Voiture 10/25\n",
      "    Foton - DH\n",
      "  Voiture 11/25\n",
      "    Fiat - DH\n",
      "  Voiture 12/25\n",
      "    Mercedes-Benz - DH\n",
      "  Voiture 13/25\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "base_url = 'https://www.avito.ma/fr/maroc/v%C3%A9hicules'\n",
    "path = r'C:\\Users\\HP830G5\\Downloads\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe'\n",
    "csv_file = r\"C:\\Users\\HP830G5\\Music\\cars_price_prediction\\data\\scrapped\\avito_cars_scrapping.csv\"\n",
    "\n",
    "def avito_scraper():\n",
    "    service = Service(executable_path=path)\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    labels = [\"Date Pub\", \"Année-Modèle\", \"Boite de vitesses\", \"Type de carburant\",\n",
    "              \"Kilométrage\", \"Marque\", \"Modèle\", \"Nombre de portes\",\n",
    "              \"Origine\", \"Première main\", \"Puissance fiscale\", \"État\", \"Prix\"]\n",
    "\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    # Mode \"a\" pour ajouter à la fin du fichier\n",
    "    with open(csv_file, \"a\", newline='', encoding=\"utf-8-sig\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=labels)\n",
    "        if not file_exists:  # écrire l'en-tête si le fichier n'existe pas\n",
    "            writer.writeheader()\n",
    "\n",
    "        for page in range(1, 50):\n",
    "            print(f\"Scraping page {page}...\")\n",
    "            url = f\"{base_url}?o={page}\"\n",
    "            driver.get(url)\n",
    "            time.sleep(4)\n",
    "\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "            all_links = soup.find_all('a', href=True)\n",
    "            links = [link for link in all_links if '/voitures_d_occasion/' in link['href']]\n",
    "\n",
    "            if not links:\n",
    "                print(\"Aucune annonce trouvée. Fin du scraping.\")\n",
    "                break\n",
    "\n",
    "            print(f\"{len(links)} annonces trouvées\")\n",
    "\n",
    "            urls, prix_list, date_pub = [], [], []\n",
    "\n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                full_url = href if href.startswith(\"https\") else \"https://www.avito.ma\" + href\n",
    "                urls.append(full_url)\n",
    "\n",
    "                prix_tag = link.find('span', string=lambda x: x and 'DH' in x)\n",
    "                if not prix_tag:\n",
    "                    parent = link.find_parent()\n",
    "                    if parent:\n",
    "                        prix_tag = parent.find('span', string=lambda x: x and 'DH' in x)\n",
    "                prix_list.append(prix_tag.text.strip() if prix_tag else \"Prix non disponible\")\n",
    "\n",
    "                date_tag = link.find('p', class_=lambda x: x and 'sc-1x0vz2r' in x)\n",
    "                if not date_tag:\n",
    "                    parent = link.find_parent()\n",
    "                    if parent:\n",
    "                        date_tag = parent.find('p', class_=lambda x: x and 'sc-1x0vz2r' in x)\n",
    "                date_pub.append(date_tag.text.strip() if date_tag else \"Date inconnue\")\n",
    "\n",
    "            data = []\n",
    "\n",
    "            for i, car_url in enumerate(urls):\n",
    "                print(f\"  Voiture {i+1}/{len(urls)}\")\n",
    "                driver.get(car_url)\n",
    "                time.sleep(3)\n",
    "\n",
    "                try:\n",
    "                    bouton = WebDriverWait(driver, 3).until(\n",
    "                        EC.presence_of_element_located((By.XPATH, \"//button[@aria-label='Voir plus']\"))\n",
    "                    )\n",
    "                    driver.execute_script(\"arguments[0].scrollIntoView(true);\", bouton)\n",
    "                    driver.execute_script(\"arguments[0].click();\", bouton)\n",
    "                    time.sleep(2)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "                html_link = driver.page_source\n",
    "                soup = BeautifulSoup(html_link, 'lxml')\n",
    "\n",
    "                blocs = soup.find_all('div', class_=lambda x: x and ('sc-cd1c365e-2' in str(x)))\n",
    "\n",
    "                infos = {label: \"\" for label in labels}\n",
    "                infos[\"Date Pub\"] = date_pub[i] if i < len(date_pub) else \"\"\n",
    "                infos[\"Prix\"] = prix_list[i] if i < len(prix_list) else \"\"\n",
    "\n",
    "                for bloc in blocs:\n",
    "                    spans = bloc.find_all('span')\n",
    "                    if len(spans) >= 2:\n",
    "                        valeur = spans[0].get_text(strip=True)\n",
    "                        label = spans[1].get_text(strip=True)\n",
    "                        if label in infos:\n",
    "                            infos[label] = valeur\n",
    "\n",
    "                data.append(infos)\n",
    "                print(f\"    {infos.get('Marque', 'N/A')} - {infos['Prix']}\")\n",
    "\n",
    "            for voiture in data:\n",
    "                writer.writerow(voiture)\n",
    "\n",
    "            f.flush()\n",
    "            print(f\"Page {page} terminée — {len(data)} voitures ajoutées.\\n\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    driver.quit()\n",
    "    print(f\"Scraping terminé. Fichier : {csv_file}\")\n",
    "    print(\"Les nouvelles voitures ont été ajoutées à la fin du fichier existant.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    avito_scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38913cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "#254468951141.2152\n",
    "#53884888659.18\n",
    "#-184250224061.4572"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
